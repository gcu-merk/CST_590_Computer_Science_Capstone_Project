#!/usr/bin/env python3
"""
Vehicle Detection Data Consolidator Service
Consumes vehicle detection results from IMX500 host service and aggregates data

This service replaces the software-based vehicle detection with a data consolidation
approach that leverages the IMX500's on-chip AI processing done by the host service.

Architecture:
IMX500 Host Service (AI Processing) -> Redis -> This Service (Data Aggregation) -> Database

Features:
- Consumes vehicle detections from Redis (generated by IMX500 host service)
- Aggregates detection statistics and patterns
- Handles data persistence and cleanup
- Provides APIs for historical vehicle data
- Zero AI processing (that's done on IMX500 chip)
"""

import time
import json
import logging
import threading
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, List, Optional, Any
import os

# Redis for consuming IMX500 AI results
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logging.error("Redis required for vehicle detection data consolidation")

# Import our Redis data models
RedisDataManager = None
VehicleDetection = None
VehicleType = None
RedisKeys = None

try:
    from redis_models import (
        VehicleDetection, VehicleType, 
        RedisDataManager, RedisKeys
    )
    logging.info("âœ… Imported Redis models from redis_models")
except ImportError:
    try:
        from edge_processing.redis_models import (
            VehicleDetection, VehicleType, 
            RedisDataManager, RedisKeys
        )
        logging.info("âœ… Imported Redis models from edge_processing.redis_models")
    except ImportError:
        logging.error("Could not import Redis models from any location")
        RedisDataManager = None

# Verify we have the required classes
if RedisDataManager is None:
    logging.error("âŒ RedisDataManager not available - consolidator cannot start")
    import sys
    sys.exit(1)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class VehicleDetectionConsolidator:
    """
    Consolidates vehicle detection data from IMX500 host service
    Focuses on data aggregation, statistics, and persistence rather than AI processing
    """
    
    def __init__(self,
                 redis_host: str = "redis",
                 redis_port: int = 6379,
                 data_retention_hours: int = 24,
                 stats_update_interval: int = 60):
        
        if not REDIS_AVAILABLE:
            raise RuntimeError("Redis required for vehicle detection data consolidation")
        
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.data_retention_hours = data_retention_hours
        self.stats_update_interval = stats_update_interval
        
        # Redis connection
        self.redis_client = None
        self.pubsub = None
        self.data_manager = None
        
        # Data aggregation
        self.running = False
        self.stats_thread = None
        self.cleanup_thread = None
        
        # Statistics tracking
        self.hourly_stats = defaultdict(lambda: {
            'total_vehicles': 0,
            'vehicle_types': defaultdict(int),
            'avg_confidence': 0.0,
            'detection_count': 0
        })
        
        self.recent_detections = deque(maxlen=1000)  # Last 1000 detections for real-time stats
        
        logger.info("Vehicle Detection Consolidator initialized")
        logger.info(f"Data retention: {data_retention_hours} hours")
        logger.info(f"Stats update interval: {stats_update_interval}s")
    
    def connect_redis(self) -> bool:
        """Connect to Redis and setup data manager"""
        try:
            self.redis_client = redis.Redis(
                host=self.redis_host,
                port=self.redis_port,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            
            # Test connection
            self.redis_client.ping()
            
            # Initialize data manager
            self.data_manager = RedisDataManager(self.redis_client)
            
            # Setup pub/sub for real-time events
            self.pubsub = self.redis_client.pubsub()
            self.pubsub.subscribe("traffic_events")
            
            logger.info(f"âœ… Connected to Redis at {self.redis_host}:{self.redis_port}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to Redis: {e}")
            return False
    
    def start(self):
        """Start the consolidation service"""
        if not self.connect_redis():
            logger.error("Cannot start without Redis connection")
            return False
        
        self.running = True
        logger.info("ðŸš— Starting Vehicle Detection Consolidator")
        
        # Start background threads
        self.stats_thread = threading.Thread(target=self._stats_update_loop, daemon=True)
        self.cleanup_thread = threading.Thread(target=self._cleanup_loop, daemon=True)
        
        self.stats_thread.start()
        self.cleanup_thread.start()
        
        # Main event processing loop
        try:
            self._process_vehicle_events()
        except KeyboardInterrupt:
            logger.info("Shutting down consolidator...")
        except Exception as e:
            logger.error(f"Consolidator error: {e}")
        finally:
            self.stop()
        
        return True
    
    def _process_vehicle_events(self):
        """Process real-time vehicle detection events from IMX500 host service"""
        logger.info("Processing vehicle detection events from IMX500...")
        
        while self.running:
            try:
                # Listen for Redis pub/sub messages
                message = self.pubsub.get_message(timeout=1.0)
                
                if message and message['type'] == 'message':
                    try:
                        event_data = json.loads(message['data'])
                        
                        if event_data.get('event_type') == 'imx500_ai_capture':
                            self._process_imx500_capture_event(event_data)
                        elif event_data.get('event_type') == 'radar_motion_detected':
                            self._process_radar_motion_event(event_data)
                            
                    except json.JSONDecodeError:
                        logger.warning("Invalid JSON in Redis message")
                    except Exception as e:
                        logger.error(f"Error processing event: {e}")
                
                # Also check for new vehicle detection keys directly
                self._check_new_vehicle_detections()
                
                time.sleep(0.1)  # Small delay to prevent CPU spinning
                
            except Exception as e:
                logger.error(f"Event processing error: {e}")
                time.sleep(1)
    
    def _process_imx500_capture_event(self, event_data: Dict[str, Any]):
        """Process IMX500 capture event with vehicle detection results"""
        try:
            vehicle_count = event_data.get('vehicle_detections', 0)
            primary_vehicle = event_data.get('primary_vehicle')
            inference_time = event_data.get('inference_time_ms', 0)
            image_id = event_data.get('image_id')
            
            if vehicle_count > 0:
                logger.info(f"ðŸš— IMX500 detected {vehicle_count} vehicles (primary: {primary_vehicle}, {inference_time:.1f}ms)")
                
                # Add to recent detections for real-time stats
                detection_summary = {
                    'timestamp': time.time(),
                    'vehicle_count': vehicle_count,
                    'primary_vehicle': primary_vehicle,
                    'inference_time_ms': inference_time,
                    'image_id': image_id,
                    'source': 'imx500_on_chip'
                }
                
                self.recent_detections.append(detection_summary)
                
                # Update hourly statistics
                hour_key = datetime.now().strftime('%Y-%m-%d_%H')
                self.hourly_stats[hour_key]['total_vehicles'] += vehicle_count
                self.hourly_stats[hour_key]['detection_count'] += 1
                
                if primary_vehicle:
                    self.hourly_stats[hour_key]['vehicle_types'][primary_vehicle] += 1
                
        except Exception as e:
            logger.error(f"Error processing IMX500 event: {e}")
    
    def _process_radar_motion_event(self, event_data: Dict[str, Any]):
        """Process radar motion detection event and trigger full data consolidation"""
        try:
            speed = event_data.get('speed', 0)
            magnitude = event_data.get('magnitude', 0)
            direction = event_data.get('direction', 'unknown')
            timestamp = event_data.get('timestamp', time.time())
            
            logger.info(f"ðŸŽ¯ Radar motion detected: {speed} mph (magnitude: {magnitude}, direction: {direction})")
            
            # Trigger comprehensive data collection
            consolidated_data = self._collect_comprehensive_data(event_data)
            
            if consolidated_data:
                logger.info(f"âœ… Comprehensive data collected for radar event")
                # TODO: Send to database persistence service when implemented
                
        except Exception as e:
            logger.error(f"Error processing radar motion event: {e}")
    
    def _collect_comprehensive_data(self, radar_event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Collect all environmental and sensor data triggered by radar motion"""
        try:
            timestamp = time.time()
            consolidation_id = f"consolidation_{int(timestamp)}"
            
            # 1. Radar data (already provided in event)
            radar_data = radar_event.copy()
            
            # 2. Get weather data (airport + DHT22)
            weather_data = self._fetch_all_weather_data()
            
            # 3. Get recent camera/IMX500 data
            camera_data = self._fetch_recent_camera_data()
            
            # 4. Create consolidated record
            consolidated_record = {
                'consolidation_id': consolidation_id,
                'timestamp': timestamp,
                'trigger_source': 'radar_motion',
                'radar_data': radar_data,
                'weather_data': weather_data,
                'camera_data': camera_data,
                'processing_notes': f"Triggered by radar detection at {radar_event.get('speed', 0)} mph"
            }
            
            # Store consolidated data in Redis for database persistence service
            self._store_consolidated_data(consolidated_record)
            
            return consolidated_record
            
        except Exception as e:
            logger.error(f"Error collecting comprehensive data: {e}")
            return None
    
    def _fetch_all_weather_data(self) -> Dict[str, Any]:
        """Fetch weather data from both airport and DHT22 sources"""
        weather_data = {}
        
        try:
            # Get airport weather data
            airport_data = self.redis_client.get('weather:airport:latest')
            if airport_data:
                weather_data['airport'] = json.loads(airport_data)
                logger.debug("âœ… Retrieved airport weather data")
        except Exception as e:
            logger.warning(f"Failed to fetch airport weather: {e}")
        
        try:
            # Get DHT22 weather data
            dht22_data = self.redis_client.get('weather:dht22:latest')
            if dht22_data:
                weather_data['dht22'] = json.loads(dht22_data)
                logger.debug("âœ… Retrieved DHT22 weather data")
        except Exception as e:
            logger.warning(f"Failed to fetch DHT22 weather: {e}")
        
        return weather_data
    
    def _fetch_recent_camera_data(self) -> Dict[str, Any]:
        """Fetch recent camera/IMX500 detection data"""
        camera_data = {}
        
        try:
            # Get latest IMX500 detection results
            latest_detection = self.redis_client.get('imx500:latest_detection')
            if latest_detection:
                camera_data['latest_detection'] = json.loads(latest_detection)
                
            # Get recent detection summary
            if self.recent_detections:
                camera_data['recent_summary'] = {
                    'count': len(self.recent_detections),
                    'latest': dict(self.recent_detections[-1]) if self.recent_detections else None
                }
                
        except Exception as e:
            logger.warning(f"Failed to fetch camera data: {e}")
        
        return camera_data
    
    def _store_consolidated_data(self, consolidated_record: Dict[str, Any]):
        """Store consolidated data for database persistence service consumption"""
        try:
            # Store latest consolidated record
            self.redis_client.set(
                'consolidation:latest',
                json.dumps(consolidated_record, default=str),
                ex=3600  # 1 hour TTL
            )
            
            # Store in time-series for historical access
            ts_key = f"consolidation:history:{consolidated_record['consolidation_id']}"
            self.redis_client.set(
                ts_key,
                json.dumps(consolidated_record, default=str),
                ex=86400  # 24 hour TTL
            )
            
            # Publish event for database persistence service
            self.redis_client.publish('database_events', json.dumps({
                'event_type': 'new_consolidated_data',
                'consolidation_id': consolidated_record['consolidation_id'],
                'timestamp': consolidated_record['timestamp']
            }))
            
            logger.info(f"ðŸ“¦ Stored consolidated data: {consolidated_record['consolidation_id']}")
            
        except Exception as e:
            logger.error(f"Failed to store consolidated data: {e}")
    
    def _check_new_vehicle_detections(self):
        """Check for new vehicle detection keys in Redis"""
        try:
            # Get all vehicle detection keys
            detection_keys = self.redis_client.keys('vehicle:detection:*')
            
            for key in detection_keys:
                try:
                    # Check if we've processed this detection
                    processed_key = f"processed:{key}"
                    if self.redis_client.exists(processed_key):
                        continue
                    
                    # Get detection data
                    detection_data = self.redis_client.get(key)
                    if detection_data:
                        detection = json.loads(detection_data)
                        self._consolidate_vehicle_detection(detection)
                        
                        # Mark as processed
                        self.redis_client.setex(processed_key, 3600, "1")  # 1 hour TTL
                        
                except Exception as e:
                    logger.warning(f"Error processing detection key {key}: {e}")
                    
        except Exception as e:
            logger.error(f"Error checking new detections: {e}")
    
    def _consolidate_vehicle_detection(self, detection_data: Dict[str, Any]):
        """Consolidate individual vehicle detection data"""
        try:
            # Convert to standardized VehicleDetection object
            vehicle_detection = VehicleDetection(
                detection_id=detection_data['detection_id'],
                timestamp=detection_data.get('timestamp', time.time()),
                vehicle_type=VehicleType(detection_data.get('vehicle_type', 'unknown')),
                confidence=detection_data.get('confidence', 0.0),
                bounding_box={
                    'x': detection_data['bounding_box']['x1'],
                    'y': detection_data['bounding_box']['y1'],
                    'width': detection_data['bounding_box']['width'],
                    'height': detection_data['bounding_box']['height']
                },
                image_id=detection_data.get('image_id', ''),
                additional_metadata={
                    'inference_source': detection_data.get('inference_source', 'imx500_on_chip'),
                    'area_pixels': detection_data.get('area_pixels', 0),
                    'center_point': detection_data.get('center_point', [0, 0]),
                    'class_id': detection_data.get('class_id', 0)
                }
            )
            
            # Store consolidated detection
            if self.data_manager:
                detection_key = self.data_manager.store_vehicle_detection(
                    vehicle_detection, 
                    vehicle_detection.image_id,
                    ttl=self.data_retention_hours * 3600
                )
                
                logger.debug(f"Consolidated vehicle detection: {detection_key}")
            
        except Exception as e:
            logger.error(f"Error consolidating detection: {e}")
    
    def _stats_update_loop(self):
        """Update statistics periodically"""
        while self.running:
            try:
                self._update_statistics()
                time.sleep(self.stats_update_interval)
            except Exception as e:
                logger.error(f"Stats update error: {e}")
                time.sleep(10)
    
    def _update_statistics(self):
        """Update and store statistics in Redis"""
        try:
            # Calculate real-time statistics from recent detections
            if not self.recent_detections:
                return
            
            current_time = time.time()
            recent_window = [d for d in self.recent_detections if current_time - d['timestamp'] < 300]  # Last 5 minutes
            
            if recent_window:
                avg_inference_time = sum(d['inference_time_ms'] for d in recent_window) / len(recent_window)
                total_vehicles = sum(d['vehicle_count'] for d in recent_window)
                
                vehicle_types = defaultdict(int)
                for detection in recent_window:
                    if detection['primary_vehicle']:
                        vehicle_types[detection['primary_vehicle']] += 1
                
                # Store real-time stats
                realtime_stats = {
                    'timestamp': current_time,
                    'window_minutes': 5,
                    'total_detections': len(recent_window),
                    'total_vehicles': total_vehicles,
                    'avg_inference_time_ms': avg_inference_time,
                    'vehicle_types': dict(vehicle_types),
                    'detections_per_minute': len(recent_window) / 5
                }
                
                self.redis_client.setex('stats:realtime:vehicles', 300, json.dumps(realtime_stats))
                
                # Store hourly stats
                hour_key = datetime.now().strftime('%Y-%m-%d_%H')
                if hour_key in self.hourly_stats:
                    hourly_data = dict(self.hourly_stats[hour_key])
                    hourly_data['vehicle_types'] = dict(hourly_data['vehicle_types'])
                    self.redis_client.setex(f'stats:hourly:vehicles:{hour_key}', 86400, json.dumps(hourly_data))
                
                logger.debug(f"Updated stats: {len(recent_window)} recent detections, {total_vehicles} vehicles")
            
        except Exception as e:
            logger.error(f"Error updating statistics: {e}")
    
    def _cleanup_loop(self):
        """Cleanup old data periodically"""
        while self.running:
            try:
                self._cleanup_old_data()
                time.sleep(3600)  # Cleanup every hour
            except Exception as e:
                logger.error(f"Cleanup error: {e}")
                time.sleep(600)  # Retry in 10 minutes
    
    def _cleanup_old_data(self):
        """Remove old vehicle detection data"""
        try:
            cutoff_time = time.time() - (self.data_retention_hours * 3600)
            
            # Clean up old detection keys
            detection_keys = self.redis_client.keys('vehicle:detection:*')
            cleaned = 0
            
            for key in detection_keys:
                try:
                    data = self.redis_client.get(key)
                    if data:
                        detection = json.loads(data)
                        if detection.get('timestamp', 0) < cutoff_time:
                            self.redis_client.delete(key)
                            cleaned += 1
                except Exception:
                    continue
            
            # Clean up old hourly stats (keep for 7 days)
            stats_cutoff = time.time() - (7 * 24 * 3600)
            stats_keys = self.redis_client.keys('stats:hourly:vehicles:*')
            
            for key in stats_keys:
                try:
                    # Extract timestamp from key
                    date_str = key.split(':')[-1]
                    hour_time = datetime.strptime(date_str, '%Y-%m-%d_%H').timestamp()
                    if hour_time < stats_cutoff:
                        self.redis_client.delete(key)
                except Exception:
                    continue
            
            if cleaned > 0:
                logger.info(f"Cleaned up {cleaned} old vehicle detections")
                
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    def get_vehicle_statistics(self, hours: int = 24) -> Dict[str, Any]:
        """Get vehicle detection statistics for the specified time period"""
        try:
            cutoff_time = time.time() - (hours * 3600)
            
            # Get recent detections
            recent = [d for d in self.recent_detections if d['timestamp'] > cutoff_time]
            
            # Calculate statistics
            total_vehicles = sum(d['vehicle_count'] for d in recent)
            vehicle_types = defaultdict(int)
            
            for detection in recent:
                if detection['primary_vehicle']:
                    vehicle_types[detection['primary_vehicle']] += 1
            
            return {
                'time_period_hours': hours,
                'total_detections': len(recent),
                'total_vehicles': total_vehicles,
                'vehicle_types': dict(vehicle_types),
                'detections_per_hour': len(recent) / hours if hours > 0 else 0,
                'avg_vehicles_per_detection': total_vehicles / len(recent) if recent else 0
            }
            
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {}
    
    def stop(self):
        """Stop the consolidation service"""
        self.running = False
        
        if self.pubsub:
            self.pubsub.close()
        
        if self.redis_client:
            self.redis_client.close()
        
        logger.info("Vehicle Detection Consolidator stopped")

def main():
    """Main entry point"""
    # Configuration
    config = {
        'redis_host': os.getenv('REDIS_HOST', 'redis'),
        'redis_port': int(os.getenv('REDIS_PORT', '6379')),
        'data_retention_hours': int(os.getenv('DATA_RETENTION_HOURS', '24')),
        'stats_update_interval': int(os.getenv('STATS_UPDATE_INTERVAL', '60'))
    }
    
    logger.info("=== Vehicle Detection Consolidator Service ===")
    logger.info("Consuming IMX500 AI results for data aggregation")
    
    try:
        consolidator = VehicleDetectionConsolidator(**config)
        return 0 if consolidator.start() else 1
        
    except Exception as e:
        logger.error(f"Service failed: {e}")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())